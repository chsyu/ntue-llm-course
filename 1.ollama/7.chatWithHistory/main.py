from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.messages import BaseMessage, trim_messages
import asyncio

LLM_MODEL = "qwen2.5:7b-instruct"
DEFAULT_SYSTEM_PROMPT = "ä½ æ˜¯ç²¾ç…‰ä¸”å¿ å¯¦çš„åŠ©æ•™ï¼Œç¦æ­¢è‡†æ¸¬ã€‚åš´ç¦ç”Ÿæˆä¸ç¬¦åˆäº‹å¯¦çš„å…§å®¹ã€‚"

# å…¨å±€èŠå¤©æ­·å²å­˜å„²ï¼ˆå¯æ›æˆ Redis/DBï¼‰
chat_histories: dict[str, InMemoryChatMessageHistory] = {}

def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    """å–å¾—/å»ºç«‹è©² session çš„æ­·å²ï¼Œä¸¦åœ¨å›å‚³å‰åšæˆªæ–·æ§åˆ¶ã€‚"""
    h = chat_histories.get(session_id)
    if h is None:
        h = InMemoryChatMessageHistory()
        chat_histories[session_id] = h
    # ä½¿ç”¨ LangChain åŸç”Ÿçš„ trim_messages é€²è¡Œæˆªæ–·
    if h.messages:
        h.messages = trim_messages(h.messages, max_tokens=24, token_counter=len, include_system=True)
    return h

def create_chat_chain():
    prompt = ChatPromptTemplate.from_messages([
        ("system", DEFAULT_SYSTEM_PROMPT),
        MessagesPlaceholder("history"),
        ("human", "{input}")
    ])
    llm = ChatOllama(model=LLM_MODEL, temperature=0.3)
    chain = prompt | llm | StrOutputParser()
    chain_with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="history",
    )
    return chain_with_history


async def chat_with_history(chain, user_input: str, session_id: str = "default"):
    config = {"configurable": {"session_id": session_id}}
    return await chain.ainvoke({"input": user_input}, config=config)

def print_welcome():
    print("=" * 50)
    print("ğŸ¤– LangChain èŠå¤©æ©Ÿå™¨äºº")
    print("=" * 50)
    print("è¼¸å…¥ 'quit' æˆ– 'exit' çµæŸå°è©±")
    print("è¼¸å…¥ 'clear' æ¸…é™¤èŠå¤©æ­·å²")
    print("è¼¸å…¥ 'history' æŸ¥çœ‹èŠå¤©æ­·å²")
    print("-" * 50)

_ROLE_MAP = {
    "system": "ç³»çµ±",
    "human": "ç”¨æˆ¶",
    "ai": "åŠ©æ‰‹",
    "tool": "å·¥å…·",
    "function": "å‡½å¼",
}

def print_chat_history(session_id: str = "default"):
    h = chat_histories.get(session_id)
    if not h or not h.messages:
        print("ç›®å‰æ²’æœ‰èŠå¤©æ­·å²ã€‚")
        return
    print("\nğŸ“‹ èŠå¤©æ­·å²:")
    print("-" * 30)
    for i, m in enumerate(h.messages, 1):
        role = _ROLE_MAP.get(m.type, m.type)
        print(f"{i}. {role}: {getattr(m, 'content', '')}")
    print("-" * 30)

def clear_chat_history(session_id: str = "default"):
    chat_histories[session_id] = InMemoryChatMessageHistory()
    print("âœ… èŠå¤©æ­·å²å·²æ¸…é™¤ã€‚")

async def main():
    print_welcome()
    chain = create_chat_chain()
    session_id = "default"
    while True:
        try:
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            low = user_input.lower()
            if low in {"quit", "exit", "é€€å‡º"}:
                print("\nğŸ‘‹ å†è¦‹ï¼")
                break
            if low in {"clear", "æ¸…é™¤"}:
                clear_chat_history(session_id); continue
            if low in {"history", "æ­·å²"}:
                print_chat_history(session_id); continue
            if not user_input:
                print("è«‹è¼¸å…¥ä¸€äº›å…§å®¹..."); continue

            print("ğŸ¤” æ­£åœ¨æ€è€ƒ...")
            resp = await chat_with_history(chain, user_input, session_id)
            print(f"\nğŸ¤– åŠ©æ‰‹: {resp}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ”¶åˆ°ä¸­æ–·è¨Šè™Ÿï¼Œå†è¦‹ï¼")
            break
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
            print("è«‹é‡è©¦...")

if __name__ == "__main__":
    asyncio.run(main())