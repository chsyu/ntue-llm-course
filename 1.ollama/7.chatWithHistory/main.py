from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnableLambda
import asyncio

LLM_MODEL = "qwen2.5:7b-instruct"
DEFAULT_SYSTEM_PROMPT = "ä½ æ˜¯ç²¾ç…‰ä¸”å¿ å¯¦çš„åŠ©æ•™ï¼Œç¦æ­¢è‡†æ¸¬ã€‚"

# å„²å­˜å¤šå€‹ session çš„å°è©±æ­·å²
chat_histories: dict[str, list] = {}

def get_session_messages(session_id: str) -> list:
    """å–å¾—æˆ–å»ºç«‹è©² session çš„å°è©±æ­·å²"""
    if session_id not in chat_histories:
        chat_histories[session_id] = [SystemMessage(content=DEFAULT_SYSTEM_PROMPT)]
    return chat_histories[session_id]

# --- Step 1: å»ºç«‹ RunnableLambdaï¼Œç”¨ä¾†ç”Ÿæˆ messages ---
def build_messages(input_dict: dict):
    """çµ„è£å®Œæ•´ messages çµ¦ LLM"""
    session_id = input_dict["session_id"]
    user_input = input_dict["input"]
    messages = get_session_messages(session_id)

    # åŠ å…¥ HumanMessage
    messages.append(HumanMessage(content=user_input))
    return messages

# --- Step 2: å®šç¾© LLM èˆ‡è¼¸å‡ºè§£æ ---
llm = ChatOllama(model=LLM_MODEL, temperature=0.7)
parser = StrOutputParser()

# --- Step 3: ç”¨ LCEL ä¸²æ¥ ---
chain = (
    RunnableLambda(build_messages)
    | llm
    | parser
)

# --- Step 4: åŸ·è¡Œå‡½æ•¸ ---
async def chat_with_history(user_input: str, session_id: str = "default"):
    # å‚³å…¥ input + session_idï¼Œè®“ build_messages æœ‰è³‡è¨Šçµ„åˆ messages
    result = await chain.ainvoke({"input": user_input, "session_id": session_id})

    # åŸ·è¡Œå®Œå¾Œ append AI å›è¦†
    messages = get_session_messages(session_id)
    messages.append(AIMessage(content=result))
    return result

def print_welcome():
    print("=" * 50)
    print("ğŸ¤– LangChain èŠå¤©æ©Ÿå™¨äºº")
    print("=" * 50)

async def main():
    print_welcome()
    session_id = "default"

    while True:
        user_input = input("\nğŸ‘¤ ä½ : ").strip()
        if user_input.lower() in {"quit", "exit", "é€€å‡º"}:
            print("\nğŸ‘‹ å†è¦‹ï¼")
            break

        print("ğŸ¤” æ­£åœ¨æ€è€ƒ...")
        resp = await chat_with_history(user_input, session_id)
        print(f"\nğŸ¤– åŠ©æ‰‹: {resp}")

if __name__ == "__main__":
    asyncio.run(main())