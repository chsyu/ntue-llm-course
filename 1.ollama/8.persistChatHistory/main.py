import json
import os
from datetime import datetime
from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
import asyncio

# ===============================
# åŸºæœ¬è¨­å®š
# ===============================
LLM_MODEL = "qwen2.5:7b-instruct"
DEFAULT_SYSTEM_PROMPT = "ä½ æ˜¯ç²¾ç…‰ä¸”å¿ å¯¦çš„åŠ©æ•™ï¼Œç¦æ­¢è‡†æ¸¬ã€‚"

# æ­·å²ç´€éŒ„è³‡æ–™å¤¾
HISTORY_DIR = "chat_histories"
os.makedirs(HISTORY_DIR, exist_ok=True)


# ===============================
# JSON æŒä¹…åŒ–æ¨¡çµ„
# ===============================
def get_history_path(session_id: str) -> str:
    """å–å¾—å°æ‡‰ session çš„æª”æ¡ˆè·¯å¾‘"""
    return os.path.join(HISTORY_DIR, f"{session_id}.json")

def load_history(session_id: str) -> list:
    """è®€å–æ­·å²ç´€éŒ„ï¼Œè‹¥ç„¡æª”æ¡ˆå‰‡å»ºç«‹æ–°æª”"""
    path = get_history_path(session_id)
    if not os.path.exists(path):
        # è‹¥ç„¡æª”æ¡ˆï¼Œå»ºç«‹å« system prompt çš„åˆå§‹æ­·å²
        init = [SystemMessage(content=DEFAULT_SYSTEM_PROMPT)]
        save_history(session_id, init)
        return init

    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # å°‡ dict è½‰ç‚º Message ç‰©ä»¶
    messages = []
    for m in data:
        role = m["type"]
        if role == "system":
            messages.append(SystemMessage(content=m["content"]))
        elif role == "human":
            messages.append(HumanMessage(content=m["content"]))
        elif role == "ai":
            messages.append(AIMessage(content=m["content"]))
    return messages

def save_history(session_id: str, messages: list):
    """å°‡ messages å„²å­˜æˆ JSON"""
    path = get_history_path(session_id)
    data = []
    for m in messages:
        data.append({
            "type": m.type,
            "content": m.content,
            "timestamp": datetime.now().isoformat()
        })
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# ===============================
# èŠå¤©é‚è¼¯
# ===============================
async def chat_with_persist(session_id: str = "default"):
    llm = ChatOllama(model=LLM_MODEL, temperature=0.7)

    messages = load_history(session_id)
    print("=" * 60)
    print(f"ğŸ—‚ï¸ è¼‰å…¥ session: {session_id}")
    print(f"ğŸ“œ æ­·å²è¨Šæ¯æ•¸é‡: {len(messages)}")
    print("=" * 60)

    while True:
        user_input = input("\nğŸ‘¤ ä½ : ").strip()
        if user_input.lower() in {"quit", "exit", "é€€å‡º"}:
            print("\nğŸ’¾ å„²å­˜ä¸­...")
            save_history(session_id, messages)
            print(f"âœ… å·²å„²å­˜æ–¼ {get_history_path(session_id)}")
            break

        # åŠ å…¥äººé¡è¨Šæ¯
        messages.append(HumanMessage(content=user_input))

        # å‘¼å« LLM
        print("ğŸ¤” åŠ©æ‰‹æ€è€ƒä¸­...")
        resp = await llm.ainvoke(messages)

        # é¡¯ç¤ºä¸¦åŠ å…¥ AI è¨Šæ¯
        print(f"\nğŸ¤– åŠ©æ‰‹: {resp.content}")
        messages.append(AIMessage(content=resp.content))

        # å¯¦æ™‚å„²å­˜
        save_history(session_id, messages)


# ===============================
# ä¸»ç¨‹å¼åŸ·è¡Œ
# ===============================
if __name__ == "__main__":
    asyncio.run(chat_with_persist("default"))